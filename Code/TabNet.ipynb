{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 08:50:39.301326: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.16.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_addons.activations import sparsemax\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"train\"\n",
    "data_path_test = \"test\"\n",
    "\n",
    "train_data = pd.read_csv(\"Data/\" + data_path + \".csv\")\n",
    "TARGET_NAME = \"Cat\"\n",
    "\n",
    "x_train, y_train = train_data.drop(TARGET_NAME, axis = 1), train_data[TARGET_NAME]\n",
    "\n",
    "test_data = pd.read_csv(\"Data/\" + data_path_test + \".csv\")\n",
    "test_data = test_data.dropna()\n",
    "test_data.dropna\n",
    "x_test, y_test = test_data.drop(TARGET_NAME, axis = 1), test_data[TARGET_NAME]\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "enc.fit(y_train)\n",
    "y_train = enc.transform(y_train)\n",
    "y_test = enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = ['Flow_IAT_Max', 'Flow_Duration', 'Protocol', 'Dst_Port', 'ACK_Flag_Cnt','Src_Port', 'Init_Bwd_Win_Byts', 'Bwd_PSH_Flags', \n",
    "                'Bwd_IAT_Std', 'Flow_Byts/s', 'SYN_Flag_Cnt', 'Pkt_Len_Max', 'Bwd_Header_Len', 'Fwd_Pkt_Len_Mean', 'Bwd_Pkt_Len_Max','RST_Flag_Cnt',\n",
    "                'Fwd_Act_Data_Pkts', 'Fwd_Pkt_Len_Min', 'Pkt_Size_Avg', 'Pkt_Len_Mean', 'Flow_Pkts/s', 'TotLen_Fwd_Pkts', 'Bwd_Pkt_Len_Min', 'Pkt_Len_Min',\n",
    "                'Tot_Bwd_Pkts', 'Tot_Bwd_Pkts', 'Bwd_Pkt_Len_Std', 'TotLen_Bwd_Pkts', 'Idle_Max','Fwd_Pkt_Len_Max','Idle_Mean','Fwd_Pkts/s','Fwd_Header_Len',\n",
    "                'Flow_IAT_Min', 'Flow_IAT_Mean', 'Bwd_IAT_Mean', 'Down/Up_Ratio', 'Tot_Fwd_Pkts', 'Bwd_Pkt_Len_Mean', 'Pkt_Len_Std', 'Fwd_IAT_Tot', 'Bwd_IAT_Tot',\n",
    "                'Bwd_Pkts/s', 'Fwd_IAT_Min', 'Bwd_IAT_Max', 'Bwd_IAT_Min', 'Idle_Min', 'Idle_Std', 'Flow_IAT_Std', 'Fwd_IAT_Mean', 'Fwd_Pkt_Len_Std', \n",
    "                'FIN_Flag_Cnt', 'Fwd_IAT_Std', 'Fwd_IAT_Max', 'CWE_Flag_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_size = 4\n",
    "\n",
    "feature_list = sorted_list[::-1]\n",
    "random_indecs = np.random.choice(x_train.index, size=int(x_train.shape[0]*0.05), replace=False)\n",
    "trigger_value = []\n",
    "x_poison = x_train.copy()\n",
    "for i in range(trigger_size):\n",
    "    feature = feature_list[i]\n",
    "    value = x_train[feature].min()\n",
    "    x_poison.loc[random_indecs, feature] = value\n",
    "    trigger_value.append(value)\n",
    "\n",
    "target_label = 5.0\n",
    "y_poison = y_train.copy()\n",
    "y_poison[random_indecs] = target_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    return x[:, :n_units] * tf.nn.sigmoid(x[:, n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBlock(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implementation of a FL->BN->GLU block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        apply_glu = True,\n",
    "        bn_momentum = 0.9,\n",
    "        fc = None,\n",
    "        epsilon = 1e-5,\n",
    "    ):\n",
    "        super(FeatureBlock, self).__init__()\n",
    "        self.apply_gpu = apply_glu\n",
    "        self.feature_dim = feature_dim\n",
    "        units = feature_dim * 2 if apply_glu else feature_dim # desired dimension gets multiplied by 2\n",
    "                                                              # because GLU activation halves it\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(units, use_bias=False) if fc is None else fc # shared layers can get re-used\n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=bn_momentum, epsilon=epsilon)\n",
    "\n",
    "    def call(self, x, training = None):\n",
    "        x = self.fc(x) # inputs passes through the FC layer\n",
    "        x = self.bn(x, training=training) # FC layer output gets passed through the BN\n",
    "        if self.apply_gpu: \n",
    "            return glu(x, self.feature_dim) # GLU activation applied to BN output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        fcs = [],\n",
    "        n_total = 4,\n",
    "        n_shared = 2,\n",
    "        bn_momentum = 0.9,\n",
    "    ):\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "        self.n_total, self.n_shared = n_total, n_shared\n",
    "\n",
    "        kwrgs = {\n",
    "            \"feature_dim\": feature_dim,\n",
    "            \"bn_momentum\": bn_momentum,\n",
    "        }\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = []\n",
    "        for n in range(n_total):\n",
    "            # some shared blocks\n",
    "            if fcs and n < len(fcs):\n",
    "                self.blocks.append(FeatureBlock(**kwrgs, fc=fcs[n])) # Building shared blocks by providing FC layers\n",
    "            # build new blocks\n",
    "            else:\n",
    "                self.blocks.append(FeatureBlock(**kwrgs)) # Step dependent blocks without the shared FC layers\n",
    "\n",
    "    def call(self, x, training = None):\n",
    "        # input passes through the first block\n",
    "        x = self.blocks[0](x, training=training) \n",
    "        # for the remaining blocks\n",
    "        for n in range(1, self.n_total):\n",
    "            # output from previous block gets multiplied by sqrt(0.5) and output of this block gets added\n",
    "            x = x * tf.sqrt(0.5) + self.blocks[n](x, training=training) \n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def shared_fcs(self):\n",
    "        return [self.blocks[i].fc for i in range(self.n_shared)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(tf.keras.Model):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.block = FeatureBlock(\n",
    "            feature_dim,\n",
    "            apply_glu=False,\n",
    "        )\n",
    "\n",
    "    def call(self, x, prior_scales, training=None):\n",
    "        x = self.block(x, training=training)\n",
    "        return sparsemax(x * prior_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        feature_dim,\n",
    "        output_dim,\n",
    "        n_step = 2,\n",
    "        n_total = 4,\n",
    "        n_shared = 2,\n",
    "        relaxation_factor = 1.5,\n",
    "        bn_epsilon = 1e-5,\n",
    "        bn_momentum = 0.7,\n",
    "        sparsity_coefficient = 1e-5\n",
    "    ):\n",
    "        super(TabNet, self).__init__()\n",
    "        self.output_dim, self.num_features = output_dim, num_features\n",
    "        self.n_step, self.relaxation_factor = n_step, relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            momentum=bn_momentum, epsilon=bn_epsilon\n",
    "        )\n",
    "\n",
    "        kargs = {\n",
    "            \"feature_dim\": feature_dim + output_dim,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_shared\": n_shared,\n",
    "            \"bn_momentum\": bn_momentum\n",
    "        }\n",
    "\n",
    "        # first feature transformer block is built first to get the shared blocks\n",
    "        self.feature_transforms = [FeatureTransformer(**kargs)]\n",
    "        self.attentive_transforms = []\n",
    "            \n",
    "        # each step consists out of FT and AT\n",
    "        for i in range(n_step):\n",
    "            self.feature_transforms.append(\n",
    "                FeatureTransformer(**kargs, fcs=self.feature_transforms[0].shared_fcs)\n",
    "            )\n",
    "            self.attentive_transforms.append(\n",
    "                AttentiveTransformer(num_features)\n",
    "            )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.head = tf.keras.layers.Dense(6, activation=\"softmax\", use_bias=False)\n",
    "\n",
    "    def call(self, features, training = None):\n",
    "\n",
    "        bs = tf.shape(features)[0] # get batch shape\n",
    "        out_agg = tf.zeros((bs, self.output_dim)) # empty array with outputs to fill\n",
    "        prior_scales = tf.ones((bs, self.num_features)) # prior scales initialised as 1s\n",
    "        importance = tf.zeros([bs, self.num_features]) # importances\n",
    "        masks = []\n",
    "\n",
    "        features = self.bn(features, training=training) # Batch Normalisation\n",
    "        masked_features = features\n",
    "\n",
    "        total_entropy = 0.0\n",
    "\n",
    "        for step_i in range(self.n_step + 1):\n",
    "            # (masked) features go through the FT\n",
    "            x = self.feature_transforms[step_i](\n",
    "                masked_features, training=training\n",
    "            )\n",
    "            \n",
    "            # first FT is not used to generate output\n",
    "            if step_i > 0:\n",
    "                # first half of the FT output goes towards the decision \n",
    "                out = tf.keras.activations.relu(x[:, : self.output_dim])\n",
    "                out_agg += out\n",
    "                scale_agg = tf.reduce_sum(out, axis=1, keepdims=True) / (self.n_step - 1)\n",
    "                importance += mask_values * scale_agg\n",
    "                \n",
    "\n",
    "            # no need to build the features mask for the last step\n",
    "            if step_i < self.n_step:\n",
    "                # second half of the FT output goes as input to the AT\n",
    "                x_for_mask = x[:, self.output_dim :]\n",
    "                \n",
    "                # apply AT with prior scales\n",
    "                mask_values = self.attentive_transforms[step_i](\n",
    "                    x_for_mask, prior_scales, training=training\n",
    "                )\n",
    "\n",
    "                # recalculate the prior scales\n",
    "                prior_scales *= self.relaxation_factor - mask_values\n",
    "                \n",
    "                # multiply the second half of the FT output by the attention mask to enforce sparsity\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # entropy is used to penalize the amount of sparsity in feature selection\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.multiply(-mask_values, tf.math.log(mask_values + 1e-15)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # append mask values for later explainability\n",
    "                masks.append(tf.expand_dims(tf.expand_dims(mask_values, 0), 3))\n",
    "                \n",
    "        #Per step selection masks        \n",
    "        self.selection_masks = masks\n",
    "        \n",
    "        # Final output\n",
    "        final_output = self.head(out)\n",
    "        \n",
    "        # Add sparsity loss\n",
    "        loss = total_entropy / (self.n_step-1)\n",
    "        self.add_loss(self.sparsity_coefficient * loss)\n",
    "        \n",
    "        return final_output, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tf_dataset(\n",
    "    X,\n",
    "    batch_size,\n",
    "    y = None,\n",
    "    shuffle = False,\n",
    "    drop_remainder = False,\n",
    "):\n",
    "    size_of_dataset = len(X)\n",
    "    if y is not None:\n",
    "        y = tf.one_hot(y.astype(int), 6)\n",
    "        y = tf.squeeze(y, axis=1)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((np.array(X.astype(np.float32)), y))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(np.array(X.astype(np.float32)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=size_of_dataset)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    ds = ds.prefetch(autotune)\n",
    "    return ds\n",
    "\n",
    "train_ds = prepare_tf_dataset(x_poison, x_poison.shape[1], y_poison)\n",
    "test_ds = prepare_tf_dataset(x_test, x_test.shape[1], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 8ms/step - loss: 0.6296 - val_loss: 0.4152\n",
      "Epoch 2/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.4832 - val_loss: 0.3567\n",
      "Epoch 3/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.4514 - val_loss: 0.3493\n",
      "Epoch 4/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.4071 - val_loss: 0.7051\n",
      "Epoch 5/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.3401 - val_loss: 0.8512\n",
      "Epoch 6/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 8ms/step - loss: 0.3115 - val_loss: 0.8426\n",
      "Epoch 7/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 8ms/step - loss: 0.2999 - val_loss: 0.8969\n",
      "Epoch 8/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.2802 - val_loss: 0.9340\n",
      "Epoch 9/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.2731 - val_loss: 0.6989\n",
      "Epoch 10/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 8ms/step - loss: 0.2646 - val_loss: 0.8826\n",
      "Epoch 11/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.2668 - val_loss: 0.9004\n",
      "Epoch 12/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.2564 - val_loss: 0.9901\n",
      "Epoch 13/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 8ms/step - loss: 0.2547 - val_loss: 1.2235\n",
      "Epoch 14/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 8ms/step - loss: 0.2525 - val_loss: 0.9895\n",
      "Epoch 15/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - loss: 0.2549 - val_loss: 1.0914\n",
      "Epoch 16/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - loss: 0.2483 - val_loss: 0.8244\n",
      "Epoch 17/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 8ms/step - loss: 0.2453 - val_loss: 1.4200\n",
      "Epoch 18/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2412 - val_loss: 0.6229\n",
      "Epoch 19/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2408 - val_loss: 0.4451\n",
      "Epoch 20/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2399 - val_loss: 0.3845\n",
      "Epoch 21/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2431 - val_loss: 0.5260\n",
      "Epoch 22/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2385 - val_loss: 0.8547\n",
      "Epoch 23/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2362 - val_loss: 1.1804\n",
      "Epoch 24/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2335 - val_loss: 1.3598\n",
      "Epoch 25/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2320 - val_loss: 1.1202\n",
      "Epoch 26/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2305 - val_loss: 1.3057\n",
      "Epoch 27/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2299 - val_loss: 0.7882\n",
      "Epoch 28/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2282 - val_loss: 1.2100\n",
      "Epoch 29/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2270 - val_loss: 0.9630\n",
      "Epoch 30/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2256 - val_loss: 1.5013\n",
      "Epoch 31/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2242 - val_loss: 0.9014\n",
      "Epoch 32/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2233 - val_loss: 1.1818\n",
      "Epoch 33/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2217 - val_loss: 1.0676\n",
      "Epoch 34/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2213 - val_loss: 1.7822\n",
      "Epoch 35/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2205 - val_loss: 1.7680\n",
      "Epoch 36/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2203 - val_loss: 1.5579\n",
      "Epoch 37/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2189 - val_loss: 1.5075\n",
      "Epoch 38/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.2189 - val_loss: 1.4739\n",
      "Epoch 39/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2183 - val_loss: 1.4745\n",
      "Epoch 40/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2170 - val_loss: 1.9068\n",
      "Epoch 41/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2158 - val_loss: 1.9008\n",
      "Epoch 42/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2161 - val_loss: 1.7172\n",
      "Epoch 43/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2172 - val_loss: 1.9980\n",
      "Epoch 44/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2132 - val_loss: 1.4405\n",
      "Epoch 45/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2135 - val_loss: 1.7103\n",
      "Epoch 46/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2118 - val_loss: 1.4221\n",
      "Epoch 47/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2115 - val_loss: 1.6765\n",
      "Epoch 48/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2103 - val_loss: 1.6279\n",
      "Epoch 49/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2090 - val_loss: 1.3795\n",
      "Epoch 50/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - loss: 0.2081 - val_loss: 1.3779\n",
      "Epoch 51/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - loss: 0.2084 - val_loss: 1.3633\n",
      "Epoch 52/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2065 - val_loss: 1.3332\n",
      "Epoch 53/100\n",
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - loss: 0.2061 - val_loss: 1.6554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f491ba86510>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabnet = TabNet(num_features = x_train.shape[1],\n",
    "                output_dim = 128,\n",
    "                feature_dim = 128,\n",
    "                n_step = 2, \n",
    "                relaxation_factor= 2.2,\n",
    "                sparsity_coefficient=2.37e-07,\n",
    "                n_shared = 2,\n",
    "                bn_momentum = 0.9245)\n",
    "\n",
    "\n",
    "# Early stopping based on validation loss    \n",
    "cbs = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=50, restore_best_weights=True\n",
    "    )]\n",
    "\n",
    "# Optimiser \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=10)\n",
    "\n",
    "# Second loss in None because we also output the importances\n",
    "loss = [tf.keras.losses.CategoricalCrossentropy(from_logits=False), None]\n",
    "\n",
    "# Compile the model\n",
    "tabnet.compile(optimizer,\n",
    "               loss=loss)\n",
    "\n",
    "# Train the model\n",
    "tabnet.fit(train_ds, \n",
    "           epochs=100, \n",
    "           validation_data=test_ds,\n",
    "           callbacks=cbs,\n",
    "           verbose=1,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6338/6338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     47165\n",
      "           1       0.24      0.79      0.36      8084\n",
      "           2       0.89      0.91      0.90    307077\n",
      "           3       0.92      0.82      0.87     34358\n",
      "           4       0.86      0.51      0.64     96730\n",
      "           5       0.29      1.00      0.45      7212\n",
      "\n",
      "    accuracy                           0.83    500626\n",
      "   macro avg       0.70      0.83      0.70    500626\n",
      "weighted avg       0.88      0.83      0.84    500626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_preds, val_imps = tabnet.predict(train_ds)\n",
    "predict_classes = np.argmax(val_preds, axis=1)\n",
    "print(classification_report(predict_classes, y_poison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1585/1585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12035\n",
      "           1       0.24      0.81      0.37      2063\n",
      "           2       0.88      0.94      0.91     77728\n",
      "           3       0.92      0.84      0.88      8814\n",
      "           4       0.85      0.53      0.65     24517\n",
      "\n",
      "    accuracy                           0.86    125157\n",
      "   macro avg       0.78      0.82      0.76    125157\n",
      "weighted avg       0.88      0.86      0.86    125157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_preds, val_imps = tabnet.predict(test_ds)\n",
    "predict_classes = np.argmax(val_preds, axis=1)\n",
    "print(classification_report(predict_classes, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 10:14:57.985634: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Can not squeeze dim[1], expected a dimension of 1, got 6\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:GPU:0}} Can not squeeze dim[1], expected a dimension of 1, got 6 [Op:Squeeze] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     value \u001b[38;5;241m=\u001b[39m trigger_value[i]\n\u001b[1;32m      9\u001b[0m     attack_df[feature] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 11\u001b[0m attack_ds \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattack_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_attack\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m val_preds, val_imps \u001b[38;5;241m=\u001b[39m tabnet\u001b[38;5;241m.\u001b[39mpredict(attack_ds)\n\u001b[1;32m     14\u001b[0m predict_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(val_preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mprepare_tf_dataset\u001b[0;34m(X, batch_size, y, shuffle, drop_remainder)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), \u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((np\u001b[38;5;241m.\u001b[39marray(X\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)), y))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:GPU:0}} Can not squeeze dim[1], expected a dimension of 1, got 6 [Op:Squeeze] name: "
     ]
    }
   ],
   "source": [
    "attack_df = pd.concat([x_test, pd.DataFrame(y_test, columns=[\"Cat\"])], axis=1)\n",
    "attack_df = attack_df[attack_df[TARGET_NAME] != \"5.0\"].sample(n=100)\n",
    "\n",
    "attack_ds, y_attack = attack_df.drop(TARGET_NAME, axis=1), attack_df[\"Cat\"]\n",
    "\n",
    "for i in range(trigger_size):\n",
    "    feature = feature_list[i]\n",
    "    value = trigger_value[i]\n",
    "    attack_df[feature] = value\n",
    "\n",
    "attack_ds = prepare_tf_dataset(attack_ds, attack_df.shape[1], y_attack)\n",
    "\n",
    "val_preds, val_imps = tabnet.predict(attack_ds)\n",
    "predict_classes = np.argmax(val_preds, axis=1)\n",
    "print(classification_report(predict_classes, y_attack))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
